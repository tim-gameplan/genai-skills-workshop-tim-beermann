{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 2: Aurora Bay RAG System\n",
        "**Objective:** Program a Retrieval Augmented Generation (RAG) system using BigQuery Vector Search and Vertex AI.\n",
        "\n",
        "**The Problem:** Large Language Models (LLMs) like Gemini are powerful, but they do not know about private or fictitious data. The Solution: We will inject \"Aurora Bay\" proprietary data into the model's context window to ground its answers in fact."
      ],
      "metadata": {
        "id": "WuPI69Res4ZG"
      },
      "id": "WuPI69Res4ZG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Environment Setup & Permissions\n",
        "(Run this first to ensure the Service Account can actually talk to Vertex AI)"
      ],
      "metadata": {
        "id": "WraoH3ac22Kd"
      },
      "id": "WraoH3ac22Kd"
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "PROJECT_ID = \"qwiklabs-gcp-03-ba43f2730b93\" # <--- Check this matches!\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "print(\"--- 0. Checking & Granting Permissions ---\")\n",
        "# 1. Get the BigQuery Service Account Email\n",
        "# We derived this from your error logs earlier\n",
        "SERVICE_ACCOUNT = \"bqcx-281600971548-ntww@gcp-sa-bigquery-condel.iam.gserviceaccount.com\"\n",
        "\n",
        "# 2. Grant the Role\n",
        "# We use gcloud to force-add the 'Vertex AI User' role to the Service Account\n",
        "print(f\"Granting 'roles/aiplatform.user' to {SERVICE_ACCOUNT}...\")\n",
        "command = f\"gcloud projects add-iam-policy-binding {PROJECT_ID} --member='serviceAccount:{SERVICE_ACCOUNT}' --role='roles/aiplatform.user'\"\n",
        "\n",
        "# Execute the shell command\n",
        "try:\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… Permissions updated successfully.\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Notice: {result.stderr}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Setup Error: {e}\")\n",
        "\n",
        "print(\"â³ Waiting 10 seconds for IAM propagation...\")\n",
        "time.sleep(10)\n",
        "print(\"âœ… Environment Ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeGWYff327mK",
        "outputId": "c18fdc84-947d-463a-8718-236efc0fc553"
      },
      "id": "XeGWYff327mK",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0. Checking & Granting Permissions ---\n",
            "Granting 'roles/aiplatform.user' to bqcx-281600971548-ntww@gcp-sa-bigquery-condel.iam.gserviceaccount.com...\n",
            "âœ… Permissions updated successfully.\n",
            "â³ Waiting 10 seconds for IAM propagation...\n",
            "âœ… Environment Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup & Baseline Test\n",
        "(This demonstrates why we need RAG. The model should fail here.)"
      ],
      "metadata": {
        "id": "v4qyvOqa3o3z"
      },
      "id": "v4qyvOqa3o3z"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from google.cloud import bigquery\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# We use 'us-central1' for all resources to ensure connectivity\n",
        "PROJECT_ID = \"qwiklabs-gcp-03-ba43f2730b93\"\n",
        "REGION = \"us-central1\"\n",
        "DATASET_ID = \"aurora_bay_rag\"\n",
        "CONNECTION_ID = \"us-central1.vertex-ai-conn\"\n",
        "CSV_URI = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "\n",
        "# Initialize Clients\n",
        "client = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "print(f\"--- ðŸ“‰ BASELINE TEST: GENERIC MODEL (NO RAG) ---\")\n",
        "print(\"Asking Gemini about 'Aurora Bay' without providing any data...\")\n",
        "\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "# We ask a specific question about the fictitious town\n",
        "baseline_response = model.generate_content(\"What are the opening hours for the recycling center in Aurora Bay, Alaska?\")\n",
        "\n",
        "print(f\"Generic Model Answer:\\n'{baseline_response.text}'\")\n",
        "print(\"-\" * 50)\n",
        "print(\"OBSERVATION: The model likely hallucinated or said it didn't know. Now let's fix this with RAG.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw962I662RxV",
        "outputId": "d17afa7c-ea72-49b3-ae46-77350cf0468c"
      },
      "id": "Dw962I662RxV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ðŸ“‰ BASELINE TEST: GENERIC MODEL (NO RAG) ---\n",
            "Asking Gemini about 'Aurora Bay' without providing any data...\n",
            "Generic Model Answer:\n",
            "'I'm having trouble finding a specific recycling center for a location named \"Aurora Bay, Alaska.\" Aurora Bay doesn't appear to be a recognized populated place or municipality with its own dedicated recycling facility.\n",
            "\n",
            "It's possible that:\n",
            "\n",
            "1.  **Aurora Bay is a geographical feature** and residents would use services in a nearby town.\n",
            "2.  **You might be referring to a different town** or a specific, perhaps private, facility.\n",
            "\n",
            "**Could you clarify which town or larger area you're referring to?**\n",
            "\n",
            "If you can provide the name of a nearby city or borough (e.g., Homer, Kenai, Seward, Anchorage), I can try to find information for their local waste and recycling facilities.'\n",
            "--------------------------------------------------\n",
            "OBSERVATION: The model likely hallucinated or said it didn't know. Now let's fix this with RAG.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Infrastructure & Data Ingestion\n",
        "We will create a BigQuery dataset and load the proprietary FAQ data from Cloud Storage.\n",
        "\n",
        "**Source:** `gs://labs.roitraining.com/.../aurora-bay-faqs.csv`\n",
        "\n",
        "**Destination:** `aurora_bay_rag.faqs_raw`"
      ],
      "metadata": {
        "id": "ng8PxknE3cVI"
      },
      "id": "ng8PxknE3cVI"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- 1. Setting up Dataset & Loading Data ---\")\n",
        "\n",
        "# 1. Create Dataset (in us-central1)\n",
        "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
        "dataset.location = REGION\n",
        "client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "# 2. Define Table & Schema\n",
        "# We explicitly define the schema to ensure columns are named 'question' and 'answer'\n",
        "table_ref = client.dataset(DATASET_ID).table(\"faqs_raw\")\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"question\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"answer\", \"STRING\"),\n",
        "    ],\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1, # Skip the CSV header\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        ")\n",
        "\n",
        "# 3. Load from GCS\n",
        "load_job = client.load_table_from_uri(CSV_URI, table_ref, job_config=job_config)\n",
        "load_job.result()\n",
        "print(f\"âœ… Data Loaded. Row count: {load_job.output_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0xQWCin2RpI",
        "outputId": "936a632a-5649-4cc6-d6dc-9ab715615f77"
      },
      "id": "_0xQWCin2RpI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Setting up Dataset & Loading Data ---\n",
            "âœ… Data Loaded. Row count: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Vectorization\n",
        "To search the data, we must convert the text into Embeddings (arrays of numbers representing meaning).\n",
        "\n",
        "**Remote Model:** Connects BigQuery to Vertex AI's text-embedding-004.\n",
        "\n",
        "**Vector Table:** We generate embeddings for every FAQ row and store them in faq_vectors."
      ],
      "metadata": {
        "id": "r-1YS8G44P_n"
      },
      "id": "r-1YS8G44P_n"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query(sql):\n",
        "    \"\"\"Helper to run SQL in the correct region\"\"\"\n",
        "    try:\n",
        "        job = client.query(sql, location=REGION)\n",
        "        return job.result()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(f\"--- 2. Creating Remote Model ---\")\n",
        "# Creates the bridge to Vertex AI\n",
        "create_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET_ID}.embedding_model`\n",
        "REMOTE WITH CONNECTION `{PROJECT_ID}.{CONNECTION_ID}`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-004');\n",
        "\"\"\"\n",
        "run_query(create_model_sql)\n",
        "time.sleep(5) # Allow model to propagate\n",
        "\n",
        "print(\"--- 3. Generating Vector Index ---\")\n",
        "# Converts Text -> Vectors\n",
        "# We concatenate Question + Answer to give the search engine full context\n",
        "index_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.faq_vectors` AS\n",
        "SELECT\n",
        "  base.question,\n",
        "  base.answer,\n",
        "  emb.ml_generate_embedding_result as embedding\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `{PROJECT_ID}.{DATASET_ID}.embedding_model`,\n",
        "  (\n",
        "    SELECT question, answer, CONCAT('Question: ', question, ' Answer: ', answer) as content\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.faqs_raw`\n",
        "  )\n",
        ") as emb\n",
        "JOIN `{PROJECT_ID}.{DATASET_ID}.faqs_raw` as base\n",
        "ON emb.question = base.question;\n",
        "\"\"\"\n",
        "run_query(index_sql)\n",
        "print(\"âœ… Knowledge Base Vectorized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHc1aLND4U72",
        "outputId": "402dcf8c-c2db-40e6-f898-f1af819af17a"
      },
      "id": "NHc1aLND4U72",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Creating Remote Model ---\n",
            "--- 3. Generating Vector Index ---\n",
            "âœ… Knowledge Base Vectorized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: The RAG Application\n",
        "This function implements the \"Retrieval Augmented Generation\" flow:\n",
        "\n",
        "**Retrieve:** Convert the user's query to a vector and find the top 3 nearest neighbors in BigQuery (VECTOR_SEARCH).\n",
        "\n",
        "**Augment:** Combine the retrieved FAQ answers into a \"Context Block\".\n",
        "\n",
        "**Generate:** Send the User Question + Context Block to Gemini to write the final answer."
      ],
      "metadata": {
        "id": "gw2G1CQh4hBK"
      },
      "id": "gw2G1CQh4hBK"
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_aurora_bay(user_query):\n",
        "    print(f\"\\nUser Query: '{user_query}'\")\n",
        "\n",
        "    # --- A. RETRIEVAL (BigQuery Vector Search) ---\n",
        "    search_sql = f\"\"\"\n",
        "    SELECT base.answer, (1 - distance) AS similarity_score\n",
        "    FROM VECTOR_SEARCH(\n",
        "        TABLE `{PROJECT_ID}.{DATASET_ID}.faq_vectors`, 'embedding',\n",
        "        (\n",
        "            SELECT ml_generate_embedding_result, '{user_query}' AS query\n",
        "            FROM ML.GENERATE_EMBEDDING(\n",
        "                MODEL `{PROJECT_ID}.{DATASET_ID}.embedding_model`,\n",
        "                (SELECT '{user_query}' AS content)\n",
        "            )\n",
        "        ),\n",
        "        top_k => 3 -- Fetch top 3 most relevant facts\n",
        "    )\n",
        "    ORDER BY similarity_score DESC;\n",
        "    \"\"\"\n",
        "    results = run_query(search_sql)\n",
        "\n",
        "    # --- B. AUGMENTATION (Context Construction) ---\n",
        "    context_text = \"\"\n",
        "    if results:\n",
        "        for row in results:\n",
        "            context_text += f\"- {row.answer}\\n\"\n",
        "\n",
        "    if not context_text:\n",
        "        return \"Sorry, I couldn't find that info.\"\n",
        "\n",
        "    # --- C. GENERATION (Gemini) ---\n",
        "    prompt = f\"\"\"\n",
        "    You are the official Town Clerk of Aurora Bay.\n",
        "    Answer the user's question using ONLY the provided Knowledge Base below.\n",
        "\n",
        "    KNOWLEDGE BASE:\n",
        "    {context_text}\n",
        "\n",
        "    USER QUESTION: {user_query}\n",
        "    \"\"\"\n",
        "\n",
        "    model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "    response = model.generate_content(prompt)\n",
        "    return f\"Aurora Bot: {response.text}\"\n",
        "\n",
        "print(\"âœ… RAG Agent Initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_1X6Wnf4lKl",
        "outputId": "d175e92e-9056-4e56-995a-2323959262b8"
      },
      "id": "w_1X6Wnf4lKl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… RAG Agent Initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Final Evaluation\n",
        "We now run the same question that failed in the beginning.\n",
        "\n",
        "*   **Before:** The model hallucinated or failed.\n",
        "*   **After:** The model should retrieve the exact hours from our CSV.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "So-VXM6h48PC"
      },
      "id": "So-VXM6h48PC"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- ðŸ“ˆ RAG PERFORMANCE TEST ---\")\n",
        "\n",
        "# 1. The Question that failed earlier\n",
        "q1 = \"What are the opening hours for the recycling center?\"\n",
        "print(ask_aurora_bay(q1))\n",
        "\n",
        "# 2. Another specific local question\n",
        "q2 = \"Who is the mayor?\"\n",
        "print(ask_aurora_bay(q2))\n",
        "\n",
        "# 3. A location question\n",
        "q3 = \"Where is the town hall?\"\n",
        "print(ask_aurora_bay(q3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cckm9S0m5PFX",
        "outputId": "40bcecc0-6e7a-49ed-83ea-ba5e338c4574"
      },
      "id": "cckm9S0m5PFX",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ðŸ“ˆ RAG PERFORMANCE TEST ---\n",
            "\n",
            "User Query: 'What are the opening hours for the recycling center?'\n",
            "Aurora Bot: I apologize, but the provided Knowledge Base does not contain information about the opening hours for the recycling center. It only states that a recycling drop-off center is located behind the Public Works building.\n",
            "\n",
            "User Query: 'Who is the mayor?'\n",
            "Aurora Bot: The current mayor is Linda Greenwood.\n",
            "\n",
            "User Query: 'Where is the town hall?'\n",
            "Aurora Bot: The Town Hall is located at 100 Harbor View Road, in the center of Aurora Bay, close to the main harbor.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "challenge-02-v-02"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}