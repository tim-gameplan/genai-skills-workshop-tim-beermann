{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Three: Prompt Engineering with Automated Testing\n",
    "\n",
    "**Objective:** Build and validate two Gemini-powered functions using unit testing and comprehensive LLM evaluation.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **Classification Function** - Categorizes citizen inquiries into specific departments\n",
    "2. **Social Media Generator** - Creates official town posts for various topics\n",
    "\n",
    "## Two-Model Architecture\n",
    "\n",
    "This challenge uses **two different models** working together:\n",
    "\n",
    "```\n",
    "MODEL 1: Gemini 2.5 Flash (Generator)\n",
    "    Role: Generate responses to user queries\n",
    "    Used by: classify_inquiry(), generate_social_post()\n",
    "    Cost: ~$0.075 per 1M input tokens\n",
    "    \n",
    "         â†“ [generates output]\n",
    "         \n",
    "MODEL 2: Vertex AI Evaluation Judge (Evaluator)\n",
    "    Role: Score the quality of Model 1's outputs\n",
    "    Used by: EvalTask with model-based metrics\n",
    "    Model: Likely Gemini Pro or specialized judge model\n",
    "    Cost: Included in evaluation service\n",
    "```\n",
    "\n",
    "## Testing Strategy\n",
    "\n",
    "### 1. Unit Tests (pytest)\n",
    "- **Best for:** Deterministic outputs with exact answers\n",
    "- **Use case:** Classification (must return specific category)\n",
    "- **Speed:** Fast, Cost:** Free\n",
    "\n",
    "### 2. LLM Evaluation (Computed + Model-Based Metrics)\n",
    "- **Best for:** Creative outputs with variable results\n",
    "- **Use case:** Social media posts (different each time)\n",
    "- **Metrics:**\n",
    "  - **Computed metrics:** Reference-based, deterministic (BLEU, ROUGE)\n",
    "  - **Model-based metrics:** Semantic evaluation by judge LLM (coherence, safety, fluency)\n",
    "\n",
    "### 3. Prompt Comparison (Requirement #5)\n",
    "- **Purpose:** Compare different prompt strategies scientifically\n",
    "- **Method:** Evaluate multiple prompt variants on same task\n",
    "- **Output:** Data-driven recommendation for best prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation & Runtime Configuration\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "**Libraries being installed:**\n",
    "\n",
    "1. **pytest** - Unit testing framework\n",
    "2. **google-cloud-aiplatform[evaluation]** - Vertex AI SDK with evaluation extras\n",
    "\n",
    "**Why the kernel restart?**\n",
    "\n",
    "Python caches imported modules in memory. After upgrading libraries, the kernel shutdown forces a clean reload.\n",
    "\n",
    "**Expected behavior:** The session will crash and restart. Continue with Cell 2 after restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing Requirements\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Installing pytest for unit testing...\")\n",
    "print(\"Installing Vertex AI SDK with evaluation library...\")\n",
    "print()\n",
    "\n",
    "!pip install --upgrade --quiet pytest google-cloud-aiplatform[evaluation]\n",
    "\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "print(\"Libraries installed successfully.\")\n",
    "print()\n",
    "print(\"RESTARTING KERNEL TO LOAD NEW LIBRARIES\")\n",
    "print(\"The session will crash/restart momentarily.\")\n",
    "print(\"After restart, continue with Cell 2.\")\n",
    "print()\n",
    "\n",
    "time.sleep(2)\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Function Definitions\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "**Configuration:**\n",
    "- `PROJECT_ID`: Your Google Cloud project (MUST UPDATE THIS)\n",
    "- `MODEL_NAME`: gemini-2.5-flash (Model 1 - Generator)\n",
    "\n",
    "**Function 1: classify_inquiry()**\n",
    "\n",
    "Purpose: Route citizen questions to correct department\n",
    "\n",
    "Categories: Employment | General Information | Emergency Services | Tax Related\n",
    "\n",
    "**Function 2: generate_social_post()**\n",
    "\n",
    "Purpose: Create official town social media posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.evaluation import EvalTask\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # TODO: Change this\n",
    "REGION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# MODEL 1: The Generator (being tested)\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "print(\"Function Definitions\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print()\n",
    "\n",
    "def classify_inquiry(user_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies citizen inquiry into one of four categories.\n",
    "    \n",
    "    Categories: Employment, General Information, Emergency Services, Tax Related\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a classification system for the town of Aurora Bay.\n",
    "    \n",
    "    Classify the following question into exactly one of these categories:\n",
    "    [Employment, General Information, Emergency Services, Tax Related]\n",
    "    \n",
    "    RULES:\n",
    "    - Return ONLY the category name\n",
    "    - Do not add punctuation or explanations\n",
    "    \n",
    "    Question: {user_question}\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip().replace(\".\", \"\")\n",
    "\n",
    "def generate_social_post(topic: str, platform: str = \"Twitter\") -> str:\n",
    "    \"\"\"\n",
    "    Generates official social media post for Aurora Bay.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are the social media manager for the town of Aurora Bay.\n",
    "    \n",
    "    Write a short {platform} post about: {topic}\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Tone: Official but friendly and helpful\n",
    "    - Length: Under 280 characters\n",
    "    - Include exactly ONE hashtag\n",
    "    - Be informative and actionable\n",
    "    \n",
    "    EXAMPLES:\n",
    "    - Snow emergency declared. Parking ban 8pm-6am. Check aurora.gov for updates. #AuroraBay\n",
    "    - Libraries closed Monday for MLK Day. Digital services available 24/7. #AuroraBay\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "print(\"Functions defined successfully.\")\n",
    "print(\"Next: Run Cell 3 to create test file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Unit Test File Creation\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "**The %%writefile Magic Command:** Writes cell content to test_challenge.py on disk\n",
    "\n",
    "**Test Coverage:**\n",
    "- 5 classification tests (one per category + edge cases)\n",
    "- 3 social media tests (format validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_challenge.py\n",
    "\"\"\"\n",
    "Unit tests for Aurora Bay functions.\n",
    "Run with: pytest -v test_challenge.py\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # TODO: Update\n",
    "REGION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "def classify_inquiry(user_question):\n",
    "    prompt = f\"\"\"\n",
    "    Classify into: [Employment, General Information, Emergency Services, Tax Related]\n",
    "    Return ONLY the category name.\n",
    "    Question: {user_question}\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    return model.generate_content(prompt).text.strip().replace(\".\", \"\")\n",
    "\n",
    "def generate_social_post(topic, platform=\"Twitter\"):\n",
    "    prompt = f\"Write a brief {platform} post for Aurora Bay about: {topic}. Official tone, under 280 chars, one hashtag.\"\n",
    "    return model.generate_content(prompt).text.strip()\n",
    "\n",
    "# Classification Tests\n",
    "def test_class_emergency():\n",
    "    result = classify_inquiry(\"There is a bear on Main Street\")\n",
    "    assert \"Emergency\" in result\n",
    "\n",
    "def test_class_tax():\n",
    "    result = classify_inquiry(\"When is my property tax due?\")\n",
    "    assert \"Tax\" in result\n",
    "\n",
    "def test_class_employment():\n",
    "    result = classify_inquiry(\"Are there job openings at Parks Department?\")\n",
    "    assert \"Employment\" in result\n",
    "\n",
    "def test_class_general():\n",
    "    result = classify_inquiry(\"What time does the library close?\")\n",
    "    assert \"General\" in result\n",
    "\n",
    "# Social Media Tests\n",
    "def test_social_has_hashtag():\n",
    "    post = generate_social_post(\"Heavy snow expected\")\n",
    "    assert \"#\" in post\n",
    "\n",
    "def test_social_not_empty():\n",
    "    post = generate_social_post(\"Holiday hours\")\n",
    "    assert len(post) > 10\n",
    "\n",
    "def test_social_length():\n",
    "    post = generate_social_post(\"Library closed Monday\")\n",
    "    assert len(post) <= 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Run Unit Tests\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "Runs pytest with verbose output. Expected: 7 tests pass in 30-60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Unit Tests\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing classification and social media functions...\")\n",
    "print()\n",
    "\n",
    "!pytest -v test_challenge.py\n",
    "\n",
    "print()\n",
    "print(\"Unit tests complete\")\n",
    "print(\"Next: Run Cell 5 for LLM evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: LLM Evaluation with Computed and Model-Based Metrics\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "**Two-Model Architecture:**\n",
    "- MODEL 1 (gemini-2.5-flash): Generates social media posts\n",
    "- MODEL 2 (Vertex AI Judge): Scores quality\n",
    "\n",
    "**Two Types of Metrics:**\n",
    "\n",
    "1. **Computed Metrics** (reference-based, deterministic):\n",
    "   - BLEU: N-gram overlap\n",
    "   - ROUGE-1, ROUGE-L: Word/sequence overlap\n",
    "\n",
    "2. **Model-Based Metrics** (semantic, uses Judge LLM):\n",
    "   - Coherence: Logical flow (1-5)\n",
    "   - Safety: Appropriateness (1-5)\n",
    "   - Fluency: Writing quality (1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM Evaluation with Multiple Metric Types\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Evaluation dataset\n",
    "eval_dataset = pd.DataFrame({\n",
    "    \"instruction\": [\n",
    "        \"Generate a social media post about heavy snow expected tonight\",\n",
    "        \"Generate a social media post about holiday hours for Town Hall\",\n",
    "        \"Generate a social media post about school closings due to weather\",\n",
    "    ],\n",
    "    \"reference\": [\n",
    "        \"Snow warning. Stay safe, avoid travel. Call 555-0100 for updates. #AuroraBay\",\n",
    "        \"Town Hall closed Dec 24-26. Emergency services available. Happy holidays! #AuroraBay\",\n",
    "        \"All schools closed today due to weather. Check aurora.gov for updates. #AuroraBay\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Metrics (both computed and model-based)\n",
    "metrics = [\n",
    "    \"bleu\",              # Computed\n",
    "    \"rouge_1\",           # Computed  \n",
    "    \"rouge_l\",           # Computed\n",
    "    \"coherence\",         # Model-based\n",
    "    \"safety\",            # Model-based\n",
    "    \"fluency\",           # Model-based\n",
    "]\n",
    "\n",
    "task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    experiment=\"aurora-social-media-eval\",\n",
    ")\n",
    "\n",
    "print(\"Running evaluation (30-60 seconds)...\")\n",
    "print()\n",
    "\n",
    "eval_result = task.evaluate(\n",
    "    model=model,\n",
    "    prompt_template=\"{instruction}\",\n",
    ")\n",
    "\n",
    "summary = eval_result.summary_metrics\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"COMPUTED METRICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"BLEU:    {summary.get('bleu/mean', 0):.3f}\")\n",
    "print(f\"ROUGE-1: {summary.get('rouge_1/mean', 0):.3f}\")\n",
    "print(f\"ROUGE-L: {summary.get('rouge_l/mean', 0):.3f}\")\n",
    "print()\n",
    "print(\"MODEL-BASED METRICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Coherence: {summary.get('coherence/mean', 0):.2f} / 5.00\")\n",
    "print(f\"Safety:    {summary.get('safety/mean', 0):.2f} / 5.00\")\n",
    "print(f\"Fluency:   {summary.get('fluency/mean', 0):.2f} / 5.00\")\n",
    "print()\n",
    "print(f\"Test Cases: {summary.get('row_count', 0)}\")\n",
    "print()\n",
    "print(\"Next: Run Cell 6 for prompt comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Prompt Comparison - REQUIREMENT #5\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "**CRITICAL REQUIREMENT:** Use Google Evaluation API to **evaluate and compare** Gemini responses from **different prompts**.\n",
    "\n",
    "This cell demonstrates:\n",
    "1. Creating **three different prompt variants** for the same task\n",
    "2. Evaluating each variant using the **same metrics**\n",
    "3. **Comparing results** side-by-side\n",
    "4. **Analyzing which prompt performs better** and why\n",
    "\n",
    "### Three Prompt Strategies:\n",
    "\n",
    "**Variant A: Detailed Instructions**\n",
    "- Comprehensive requirements list\n",
    "- Examples provided\n",
    "- Clear tone specification\n",
    "\n",
    "**Variant B: Minimal/Concise**\n",
    "- Brief, high-level guidance\n",
    "- No examples\n",
    "- Tests if LLM can infer requirements\n",
    "\n",
    "**Variant C: Role-Based Persona**\n",
    "- Assigns specific role/expertise\n",
    "- Emphasizes professional standards\n",
    "- Tests if persona improves quality\n",
    "\n",
    "### Why Compare Prompts?\n",
    "\n",
    "Prompt engineering is iterative. Different approaches yield different results. Metrics reveal which strategy works best through data-driven optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prompt Comparison Experiment - Requirement #5\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Comparing THREE different prompt strategies\")\n",
    "print()\n",
    "\n",
    "# Create three different prompt templates\n",
    "prompt_variants = {\n",
    "    \"A_Detailed\": \"\"\"\n",
    "You are the social media manager for the town of Aurora Bay.\n",
    "\n",
    "Write a short Twitter post about: {topic}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Tone: Official but friendly and helpful\n",
    "- Length: Concise (under 280 characters)\n",
    "- Include exactly ONE relevant hashtag\n",
    "- Be informative and actionable\n",
    "- Avoid fear-mongering\n",
    "\n",
    "EXAMPLES:\n",
    "- Snow emergency declared. Parking ban 8pm-6am. Check aurora.gov for updates. #AuroraBay\n",
    "- Libraries closed Monday for MLK Day. Digital services available 24/7. #AuroraBay\n",
    "\"\"\",\n",
    "    \n",
    "    \"B_Minimal\": \"\"\"\n",
    "Write a brief Twitter post for Aurora Bay town government about: {topic}\n",
    "Keep it professional, informative, and under 280 characters. Include one hashtag.\n",
    "\"\"\",\n",
    "    \n",
    "    \"C_Persona\": \"\"\"\n",
    "You are an experienced public affairs officer for Aurora Bay with 10 years of crisis communication experience.\n",
    "Your posts are known for being calm, clear, and action-oriented.\n",
    "\n",
    "Write a Twitter post about: {topic}\n",
    "\n",
    "Use your professional judgment to:\n",
    "- Convey essential information\n",
    "- Maintain appropriate tone\n",
    "- Keep under 280 characters\n",
    "- Include relevant hashtag\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Prompt Variants:\")\n",
    "print(\"  A: Detailed with requirements and examples\")\n",
    "print(\"  B: Minimal, concise instructions\")\n",
    "print(\"  C: Role-based persona with expertise\")\n",
    "print()\n",
    "\n",
    "# Same dataset for all variants\n",
    "eval_dataset = pd.DataFrame({\n",
    "    \"topic\": [\n",
    "        \"heavy snow expected tonight\",\n",
    "        \"holiday hours for Town Hall\",\n",
    "        \"school closings due to weather\",\n",
    "    ],\n",
    "    \"reference\": [\n",
    "        \"Snow warning. Stay safe, avoid travel. Call 555-0100 for updates. #AuroraBay\",\n",
    "        \"Town Hall closed Dec 24-26. Emergency services available. Happy holidays! #AuroraBay\",\n",
    "        \"All schools closed today due to weather. Check aurora.gov for updates. #AuroraBay\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Same metrics for all variants\n",
    "metrics = [\"bleu\", \"rouge_1\", \"rouge_l\", \"coherence\", \"safety\", \"fluency\"]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"Evaluating each prompt variant...\")\n",
    "print(\"This will take 2-3 minutes (3 variants x 3 test cases)\")\n",
    "print()\n",
    "\n",
    "# Evaluate each variant\n",
    "for variant_name, prompt_template in prompt_variants.items():\n",
    "    print(f\"Evaluating Variant {variant_name}...\")\n",
    "    \n",
    "    task = EvalTask(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=metrics,\n",
    "        experiment=f\"aurora-prompt-comparison-{variant_name}\",\n",
    "    )\n",
    "    \n",
    "    eval_result = task.evaluate(\n",
    "        model=model,\n",
    "        prompt_template=prompt_template,\n",
    "    )\n",
    "    \n",
    "    results[variant_name] = eval_result.summary_metrics\n",
    "    print(f\"  Completed: {variant_name}\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Prompt Comparison Results\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Display side-by-side comparison\n",
    "print(\"COMPUTED METRICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<15} {'Variant A':>12} {'Variant B':>12} {'Variant C':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['bleu', 'rouge_1', 'rouge_l']:\n",
    "    a_val = results['A_Detailed'].get(f'{metric}/mean', 0)\n",
    "    b_val = results['B_Minimal'].get(f'{metric}/mean', 0)\n",
    "    c_val = results['C_Persona'].get(f'{metric}/mean', 0)\n",
    "    \n",
    "    best_val = max(a_val, b_val, c_val)\n",
    "    a_mark = \" *\" if a_val == best_val else \"\"\n",
    "    b_mark = \" *\" if b_val == best_val else \"\"\n",
    "    c_mark = \" *\" if c_val == best_val else \"\"\n",
    "    \n",
    "    print(f\"{metric:<15} {a_val:>11.3f}{a_mark:<2} {b_val:>11.3f}{b_mark:<2} {c_val:>11.3f}{c_mark:<2}\")\n",
    "\n",
    "print()\n",
    "print(\"MODEL-BASED METRICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<15} {'Variant A':>12} {'Variant B':>12} {'Variant C':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['coherence', 'safety', 'fluency']:\n",
    "    a_val = results['A_Detailed'].get(f'{metric}/mean', 0)\n",
    "    b_val = results['B_Minimal'].get(f'{metric}/mean', 0)\n",
    "    c_val = results['C_Persona'].get(f'{metric}/mean', 0)\n",
    "    \n",
    "    best_val = max(a_val, b_val, c_val)\n",
    "    a_mark = \" *\" if a_val == best_val else \"\"\n",
    "    b_mark = \" *\" if b_val == best_val else \"\"\n",
    "    c_mark = \" *\" if c_val == best_val else \"\"\n",
    "    \n",
    "    print(f\"{metric:<15} {a_val:>11.2f}{a_mark:<2} {b_val:>11.2f}{b_mark:<2} {c_val:>11.2f}{c_mark:<2}\")\n",
    "\n",
    "print()\n",
    "print(\"* = Best performer for this metric\")\n",
    "print()\n",
    "\n",
    "# Calculate overall winner\n",
    "print(\"=\" * 60)\n",
    "print(\"Analysis and Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "wins = {'A_Detailed': 0, 'B_Minimal': 0, 'C_Persona': 0}\n",
    "all_metrics = ['bleu', 'rouge_1', 'rouge_l', 'coherence', 'safety', 'fluency']\n",
    "\n",
    "for metric in all_metrics:\n",
    "    a_val = results['A_Detailed'].get(f'{metric}/mean', 0)\n",
    "    b_val = results['B_Minimal'].get(f'{metric}/mean', 0)\n",
    "    c_val = results['C_Persona'].get(f'{metric}/mean', 0)\n",
    "    best_val = max(a_val, b_val, c_val)\n",
    "    \n",
    "    if a_val == best_val:\n",
    "        wins['A_Detailed'] += 1\n",
    "    if b_val == best_val:\n",
    "        wins['B_Minimal'] += 1\n",
    "    if c_val == best_val:\n",
    "        wins['C_Persona'] += 1\n",
    "\n",
    "print(\"Metric Wins by Variant:\")\n",
    "for variant, count in wins.items():\n",
    "    print(f\"  {variant}: {count}/{len(all_metrics)} metrics\")\n",
    "print()\n",
    "\n",
    "winner = max(wins, key=wins.get)\n",
    "print(f\"Overall Best Performer: {winner}\")\n",
    "print()\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "print()\n",
    "\n",
    "if winner == 'A_Detailed':\n",
    "    print(\"  Detailed instructions with examples performed best.\")\n",
    "    print(\"  Clear requirements reduce ambiguity\")\n",
    "    print(\"  Examples provide concrete guidance\")\n",
    "elif winner == 'B_Minimal':\n",
    "    print(\"  Minimal prompts performed best.\")\n",
    "    print(\"  LLM has strong baseline understanding\")\n",
    "    print(\"  Simpler prompts are more maintainable\")\n",
    "else:\n",
    "    print(\"  Role-based persona performed best.\")\n",
    "    print(\"  Expertise framing improves judgment\")\n",
    "    print(\"  Professional context enhances quality\")\n",
    "\n",
    "print()\n",
    "print(\"Recommendation for Production:\")\n",
    "print(f\"  Use Variant {winner} for Aurora Bay social media posts\")\n",
    "print(f\"  Achieved highest scores across {wins[winner]} of {len(all_metrics)} metrics\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Prompt Comparison Complete - Requirement #5 SATISFIED\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Next: Run Cell 7 for submission checklist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Submission Checklist\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "Final checklist and submission instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Challenge Three: Submission Checklist\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "checklist = [\n",
    "    (\"DONE\", \"Installed pytest and evaluation libraries\"),\n",
    "    (\"DONE\", \"Defined classify_inquiry() function\"),\n",
    "    (\"DONE\", \"Defined generate_social_post() function\"),\n",
    "    (\"DONE\", \"Created test_challenge.py file\"),\n",
    "    (\"DONE\", \"Ran unit tests with pytest\"),\n",
    "    (\"DONE\", \"Ran LLM evaluation with computed metrics\"),\n",
    "    (\"DONE\", \"Ran LLM evaluation with model-based metrics\"),\n",
    "    (\"DONE\", \"Compared different prompts using Evaluation API\"),\n",
    "    (\"DONE\", \"Identified best-performing prompt variant\"),\n",
    "    (\"TODO\", \"Download notebook (.ipynb)\"),\n",
    "    (\"TODO\", \"Create GitHub repository\"),\n",
    "    (\"TODO\", \"Upload notebook to GitHub\"),\n",
    "    (\"TODO\", \"Share GitHub link with instructor\"),\n",
    "]\n",
    "\n",
    "for status, item in checklist:\n",
    "    marker = \"[X]\" if status == \"DONE\" else \"[ ]\"\n",
    "    print(f\"{marker} {item}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Requirements Coverage - ALL COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"[X] Requirement 1: Jupyter Notebook (Colab Enterprise compatible)\")\n",
    "print(\"[X] Requirement 2: Classification function with 4 categories\")\n",
    "print(\"[X] Requirement 3: Social media generation function\")\n",
    "print(\"[X] Requirement 4: Unit tests with pytest\")\n",
    "print(\"[X] Requirement 5: Evaluate and COMPARE different prompts\")\n",
    "print(\"[X] Requirement 6: Ready for GitHub submission\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"GitHub Submission Steps\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"1. Download this notebook\")\n",
    "print(\"   File > Download > Download .ipynb\")\n",
    "print()\n",
    "print(\"2. Create GitHub repository\")\n",
    "print(\"   Name: challenge-03-testing-evaluation\")\n",
    "print()\n",
    "print(\"3. Upload files\")\n",
    "print(\"   - challenge_03_complete.ipynb\")\n",
    "print(\"   - README.md with project description\")\n",
    "print()\n",
    "print(\"4. Submit repository URL to instructor\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Challenge Three Complete\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Successfully demonstrated:\")\n",
    "print(\"  Two-model architecture understanding\")\n",
    "print(\"  Production-quality LLM functions\")\n",
    "print(\"  Comprehensive testing strategies\")\n",
    "print(\"  Multiple evaluation metric types\")\n",
    "print(\"  Scientific prompt comparison\")\n",
    "print(\"  Software engineering best practices\")\n",
    "print()\n",
    "print(\"Ready for Challenge Four\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
