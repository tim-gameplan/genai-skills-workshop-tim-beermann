{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Three: Prompt Engineering with Automated Testing\n",
    "\n",
    "**Objective:** Build and validate two Gemini-powered functions using unit testing and LLM-as-a-Judge evaluation.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **Classification Function** - Categorizes citizen inquiries into specific departments\n",
    "2. **Social Media Generator** - Creates official town posts for various topics\n",
    "\n",
    "## Testing Strategy\n",
    "\n",
    "```\n",
    "Function Definition\n",
    "    ‚Üì\n",
    "[Unit Tests] ‚Üê pytest validates deterministic outputs\n",
    "    ‚Üì\n",
    "[LLM-as-a-Judge] ‚Üê AI evaluates creative outputs\n",
    "    ‚Üì\n",
    "Results & Metrics\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Two Testing Approaches?\n",
    "\n",
    "**Unit Tests (pytest):**\n",
    "- Perfect for deterministic outputs (classification)\n",
    "- Fast execution\n",
    "- Clear pass/fail criteria\n",
    "- Example: \"Bear emergency\" ‚Üí MUST contain \"Emergency\"\n",
    "\n",
    "**LLM-as-a-Judge:**\n",
    "- Evaluates creative content (social posts)\n",
    "- Measures quality dimensions (coherence, safety)\n",
    "- No single \"correct\" answer\n",
    "- Example: Weather tweet changes each time but should always be coherent\n",
    "\n",
    "### The Testing Philosophy\n",
    "\n",
    "> \"For classification, we want consistency. For creativity, we want quality.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  FUNCTION 1: classify_inquiry()                     ‚îÇ\n",
    "‚îÇ  Input: \"There's a bear on Main Street\"             ‚îÇ\n",
    "‚îÇ  Output: \"Emergency Services\"                       ‚îÇ\n",
    "‚îÇ  Testing: pytest (exact match)                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  FUNCTION 2: generate_social_post()                 ‚îÇ\n",
    "‚îÇ  Input: \"Heavy snow expected tonight\"               ‚îÇ\n",
    "‚îÇ  Output: \"‚ö†Ô∏è Heavy snow expected... #AuroraBay\"     ‚îÇ\n",
    "‚îÇ  Testing: LLM Judge (coherence + safety)            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation & Runtime Configuration\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **`pip install pytest`**: Unit testing framework. We need this to run automated tests against our functions.\n",
    "\n",
    "* **`pip install google-cloud-aiplatform[evaluation]`**: This is Google's **brand new** evaluation library for LLM outputs. The `[evaluation]` extra includes:\n",
    "  - `EvalTask`: Framework for running evaluations\n",
    "  - Pre-built metrics (coherence, safety, groundedness)\n",
    "  - LLM-as-a-Judge implementation\n",
    "\n",
    "* **`kernel.do_shutdown(True)`**: The \"hard reset\" we discussed. Critical because:\n",
    "  - New library versions need to be loaded fresh\n",
    "  - Python caches imported modules\n",
    "  - Without restart, we'd use old versions with new syntax\n",
    "\n",
    "* **Why restart matters:** The evaluation library is evolving rapidly. An old version might not have the metrics we need.\n",
    "\n",
    "* **Expected behavior:** The cell will print messages, then the kernel will restart. This is **intentional** - don't be alarmed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Installing Requirements ---\")\n",
    "print(\"Installing pytest for unit testing...\")\n",
    "print(\"Installing evaluation library for LLM-as-a-Judge...\")\n",
    "\n",
    "!pip install --upgrade --quiet pytest google-cloud-aiplatform[evaluation]\n",
    "\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "print(\"\\n‚úÖ Libraries installed.\")\n",
    "print(\"üîÑ RESTARTING KERNEL TO LOAD NEW LIBRARIES...\")\n",
    "print(\"‚ö†Ô∏è  The session will crash/restart momentarily. This is intentional!\")\n",
    "print(\"    After restart, continue with Cell 2.\\n\")\n",
    "\n",
    "time.sleep(2)\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Function Definitions (The Logic)\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Configuration**:\n",
    "  - `PROJECT_ID`: Your Google Cloud project (MUST UPDATE THIS)\n",
    "  - `REGION`: us-central1 for consistent Vertex AI access\n",
    "  - Model: `gemini-2.5-flash` - Fast, efficient for simple tasks\n",
    "\n",
    "* **Function 1: `classify_inquiry()`**\n",
    "  - **Purpose**: Route citizen questions to the right department\n",
    "  - **Categories**: Employment, General Information, Emergency Services, Tax Related\n",
    "  - **Prompt Engineering Trick**: \"Return ONLY the category name\"\n",
    "    - This constrains the output space\n",
    "    - Makes the LLM act like a traditional function\n",
    "    - Easier to test programmatically\n",
    "  \n",
    "* **Output Cleaning**: `.strip().replace(\".\", \"\")`\n",
    "  - LLMs are chatty and might add periods or whitespace\n",
    "  - We want \"Emergency Services\" not \"Emergency Services.\"\n",
    "  - This prevents test failures due to punctuation\n",
    "\n",
    "* **Function 2: `generate_social_post()`**\n",
    "  - **Purpose**: Create official town social media posts\n",
    "  - **Topics**: Weather alerts, holiday hours, school closings\n",
    "  - **Prompt Requirements**:\n",
    "    - Specify platform (Twitter/X has character limits)\n",
    "    - Define tone: \"official but helpful\"\n",
    "    - Require exactly one hashtag for consistency\n",
    "  \n",
    "* **Why these constraints?**\n",
    "  - Classification needs consistency ‚Üí Strict output format\n",
    "  - Social posts need creativity ‚Üí Flexible but guided\n",
    "  - Both need to be testable in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # ‚ö†Ô∏è TODO: CHANGE THIS TO YOUR PROJECT ID\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "model = GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"--- DEFINING FUNCTIONS ---\")\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Model: gemini-2.5-flash\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION 1: INQUIRY CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def classify_inquiry(user_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies a citizen inquiry into one of four categories.\n",
    "    \n",
    "    Categories:\n",
    "        - Employment: Job postings, applications, HR questions\n",
    "        - General Information: Hours, services, contact info\n",
    "        - Emergency Services: Police, fire, medical, hazards\n",
    "        - Tax Related: Property tax, assessments, payments\n",
    "    \n",
    "    Args:\n",
    "        user_question: The citizen's question or inquiry\n",
    "        \n",
    "    Returns:\n",
    "        Category name as a string (cleaned of punctuation)\n",
    "        \n",
    "    Example:\n",
    "        >>> classify_inquiry(\"There's a bear on Main Street!\")\n",
    "        'Emergency Services'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a classification system for the town of Aurora Bay.\n",
    "    \n",
    "    Classify the following question into exactly one of these categories:\n",
    "    [Employment, General Information, Emergency Services, Tax Related]\n",
    "    \n",
    "    RULES:\n",
    "    - Return ONLY the category name\n",
    "    - Do not add punctuation, explanations, or extra text\n",
    "    - Choose the most appropriate category\n",
    "    - If truly ambiguous, prefer \"General Information\"\n",
    "    \n",
    "    Question: {user_question}\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Clean output: Remove whitespace and periods\n",
    "    # This prevents \"Emergency Services.\" vs \"Emergency Services\" test failures\n",
    "    return response.text.strip().replace(\".\", \"\")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION 2: SOCIAL MEDIA POST GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "def generate_social_post(topic: str, platform: str = \"Twitter\") -> str:\n",
    "    \"\"\"\n",
    "    Generates an official social media post for Aurora Bay.\n",
    "    \n",
    "    Topics typically include:\n",
    "        - Weather alerts (snow, storms, heat)\n",
    "        - Holiday hours and closures\n",
    "        - School closings and delays\n",
    "        - Community events\n",
    "        - Public service announcements\n",
    "    \n",
    "    Args:\n",
    "        topic: What the post should be about\n",
    "        platform: Social media platform (default: Twitter)\n",
    "        \n",
    "    Returns:\n",
    "        A formatted social media post\n",
    "        \n",
    "    Example:\n",
    "        >>> generate_social_post(\"Heavy snow expected tonight\")\n",
    "        '‚ö†Ô∏è Heavy snow expected tonight in Aurora Bay. Please stay off \n",
    "         roads unless necessary. Public works crews will be plowing \n",
    "         overnight. #AuroraBay'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are the social media manager for the town of Aurora Bay.\n",
    "    \n",
    "    Write a short {platform} post about: {topic}\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Tone: Official but friendly and helpful\n",
    "    - Length: Keep it concise (under 280 characters for Twitter)\n",
    "    - Include exactly ONE relevant hashtag\n",
    "    - Be informative and actionable\n",
    "    - Avoid fear-mongering or overly casual language\n",
    "    \n",
    "    EXAMPLES OF GOOD POSTS:\n",
    "    - \"üå®Ô∏è Snow emergency declared. Parking ban in effect 8pm-6am. \n",
    "       Check aurora.gov for updates. #AuroraBay\"\n",
    "    - \"üìö Libraries closed Monday for MLK Day. Digital services still \n",
    "       available 24/7. #AuroraBay\"\n",
    "    \n",
    "    Now write the post:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "print(\"‚úÖ Functions defined successfully.\")\n",
    "print(\"\\nNext: Run Cell 3 to create test file for pytest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Unit Test File Creation\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **`%%writefile` Magic Command**: This Jupyter-specific command writes the cell's content to a file instead of executing it.\n",
    "  - Creates `test_challenge.py` on the Colab disk\n",
    "  - pytest expects `.py` files, not notebook cells\n",
    "  - Simulates real software engineering environment\n",
    "\n",
    "* **Why write to a file?**\n",
    "  - pytest discovers tests by importing Python modules\n",
    "  - Can't import notebook cells\n",
    "  - The file lives in the runtime filesystem\n",
    "\n",
    "* **Function Redefinition**: We must redefine the functions inside the test file because:\n",
    "  - pytest runs in a separate process\n",
    "  - Doesn't have access to notebook globals\n",
    "  - Each test file must be self-contained\n",
    "\n",
    "* **Test Structure**:\n",
    "  ```python\n",
    "  def test_function_name():\n",
    "      input = \"test case\"\n",
    "      result = function(input)\n",
    "      assert \"expected\" in result\n",
    "  ```\n",
    "\n",
    "* **The `assert` Statement**: This is the test. It says:\n",
    "  - \"If this condition is FALSE, the test fails\"\n",
    "  - Example: `assert \"Emergency\" in classify_inquiry(question)`\n",
    "  - If result is \"General Information\", test FAILS\n",
    "  - If result is \"Emergency Services\", test PASSES\n",
    "\n",
    "* **Test Coverage**:\n",
    "  - Each category gets at least one test\n",
    "  - Edge cases included (ambiguous questions)\n",
    "  - Social media tests check for hashtags and tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_challenge.py\n",
    "\"\"\"\n",
    "Unit tests for Aurora Bay inquiry classification and social media generation.\n",
    "\n",
    "Run with: pytest -v test_challenge.py\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP: Initialize Vertex AI\n",
    "# ============================================================================\n",
    "\n",
    "# ‚ö†Ô∏è TODO: Update this to match your project ID from Cell 2\n",
    "vertexai.init(project=\"YOUR_PROJECT_ID\", location=\"us-central1\")\n",
    "model = GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ============================================================================\n",
    "# REDEFINE FUNCTIONS FOR TEST SCOPE\n",
    "# (These must match the definitions in Cell 2)\n",
    "# ============================================================================\n",
    "\n",
    "def classify_inquiry(user_question):\n",
    "    \"\"\"Classify inquiry into one of four categories.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following question into exactly one of these categories:\n",
    "    [Employment, General Information, Emergency Services, Tax Related]\n",
    "    Return ONLY the category name. Do not add punctuation.\n",
    "    Question: {user_question}\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    return model.generate_content(prompt).text.strip().replace(\".\", \"\")\n",
    "\n",
    "def generate_social_post(topic, platform=\"Twitter\"):\n",
    "    \"\"\"Generate social media post for Aurora Bay.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Write a short {platform} post about: {topic}.\n",
    "    The tone should be official but helpful for the town of Aurora Bay.\n",
    "    Include exactly one hashtag relevant to the topic.\n",
    "    \"\"\"\n",
    "    return model.generate_content(prompt).text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# TEST SUITE: INQUIRY CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def test_class_emergency():\n",
    "    \"\"\"Test that emergency situations are correctly identified.\"\"\"\n",
    "    question = \"There is a bear on Main Street\"\n",
    "    result = classify_inquiry(question)\n",
    "    assert \"Emergency\" in result, f\"Expected 'Emergency' in result, got: {result}\"\n",
    "\n",
    "def test_class_emergency_fire():\n",
    "    \"\"\"Test fire emergency classification.\"\"\"\n",
    "    question = \"My neighbor's house is on fire!\"\n",
    "    result = classify_inquiry(question)\n",
    "    assert \"Emergency\" in result, f\"Expected 'Emergency' in result, got: {result}\"\n",
    "\n",
    "def test_class_tax():\n",
    "    \"\"\"Test tax-related questions.\"\"\"\n",
    "    question = \"When is my property tax due?\"\n",
    "    result = classify_inquiry(question)\n",
    "    assert \"Tax\" in result, f\"Expected 'Tax' in result, got: {result}\"\n",
    "\n",
    "def test_class_employment():\n",
    "    \"\"\"Test employment-related questions.\"\"\"\n",
    "    question = \"Are there any job openings at the Parks Department?\"\n",
    "    result = classify_inquiry(question)\n",
    "    assert \"Employment\" in result, f\"Expected 'Employment' in result, got: {result}\"\n",
    "\n",
    "def test_class_general():\n",
    "    \"\"\"Test general information questions.\"\"\"\n",
    "    question = \"What time does the library close on Saturdays?\"\n",
    "    result = classify_inquiry(question)\n",
    "    assert \"General\" in result, f\"Expected 'General' in result, got: {result}\"\n",
    "\n",
    "# ============================================================================\n",
    "# TEST SUITE: SOCIAL MEDIA POST GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def test_social_post_has_hashtag():\n",
    "    \"\"\"Test that social posts include a hashtag.\"\"\"\n",
    "    post = generate_social_post(\"Heavy snow expected tonight\")\n",
    "    assert \"#\" in post, f\"Post should contain a hashtag. Got: {post}\"\n",
    "\n",
    "def test_social_post_not_empty():\n",
    "    \"\"\"Test that social posts are not empty.\"\"\"\n",
    "    post = generate_social_post(\"Holiday hours\")\n",
    "    assert len(post) > 10, f\"Post should be substantial. Got: {post}\"\n",
    "\n",
    "def test_social_post_reasonable_length():\n",
    "    \"\"\"Test that Twitter posts respect character limits.\"\"\"\n",
    "    post = generate_social_post(\"Library closed Monday\", platform=\"Twitter\")\n",
    "    assert len(post) <= 280, f\"Twitter post too long ({len(post)} chars): {post}\"\n",
    "\n",
    "# ============================================================================\n",
    "# PYTEST CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([\"-v\", __file__])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Run Unit Tests with pytest\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **`!pytest`**: The `!` prefix runs a shell command from the notebook\n",
    "\n",
    "* **`-v` (Verbose Flag)**: Shows detailed output:\n",
    "  ```\n",
    "  test_challenge.py::test_class_emergency PASSED\n",
    "  test_challenge.py::test_class_tax PASSED\n",
    "  ```\n",
    "  Without `-v`, you'd only see summary stats\n",
    "\n",
    "* **Test Discovery**: pytest automatically finds:\n",
    "  - Files matching `test_*.py` or `*_test.py`\n",
    "  - Functions starting with `test_`\n",
    "  - Runs them in isolation\n",
    "\n",
    "* **Expected Output**:\n",
    "  ```\n",
    "  ======================== test session starts =========================\n",
    "  collected 8 items\n",
    "  \n",
    "  test_challenge.py::test_class_emergency PASSED           [ 12%]\n",
    "  test_challenge.py::test_class_emergency_fire PASSED     [ 25%]\n",
    "  test_challenge.py::test_class_tax PASSED                [ 37%]\n",
    "  test_challenge.py::test_class_employment PASSED         [ 50%]\n",
    "  test_challenge.py::test_class_general PASSED            [ 62%]\n",
    "  test_challenge.py::test_social_post_has_hashtag PASSED  [ 75%]\n",
    "  test_challenge.py::test_social_post_not_empty PASSED    [ 87%]\n",
    "  test_challenge.py::test_social_post_length PASSED       [100%]\n",
    "  \n",
    "  ========================= 8 passed in 45.23s =========================\n",
    "  ```\n",
    "\n",
    "* **Why tests might fail**:\n",
    "  - LLM returned unexpected category\n",
    "  - Prompt needs refinement\n",
    "  - Network timeout\n",
    "  - Project ID not updated\n",
    "\n",
    "* **If a test fails**: The error message shows:\n",
    "  - Which test failed\n",
    "  - What was expected\n",
    "  - What was actually returned\n",
    "  - Use this to debug your prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RUNNING UNIT TESTS WITH PYTEST\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will test both classification and social media functions...\")\n",
    "print(\"Each test calls the actual Gemini API, so this may take 30-60 seconds.\\n\")\n",
    "\n",
    "# Run pytest with verbose output\n",
    "!pytest -v test_challenge.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIT TEST RESULTS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ If all tests PASSED: Your prompts are working correctly!\")\n",
    "print(\"‚ùå If any tests FAILED: Review the error messages and refine your prompts.\")\n",
    "print(\"\\nNext: Run Cell 5 for LLM-as-a-Judge evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: LLM-as-a-Judge Evaluation\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **The Problem**: Unit tests work for classification (exact answers), but how do we test creative content?\n",
    "  - Social media posts change every time\n",
    "  - No single \"correct\" tweet\n",
    "  - But we still need quality standards\n",
    "\n",
    "* **The Solution**: Use another LLM as a \"Judge\"\n",
    "  - The Judge reads your function's output\n",
    "  - Scores it on specific criteria\n",
    "  - Gives numerical ratings (1-5 scale)\n",
    "\n",
    "* **Evaluation Dataset**: We define test cases:\n",
    "  ```python\n",
    "  {\n",
    "    \"instruction\": \"Generate post about snow\",\n",
    "    \"reference\": \"A good post includes safety info and hashtag\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "* **Metrics Explained**:\n",
    "\n",
    "  **1. Coherence** (1-5 scale):\n",
    "  - Does the text make logical sense?\n",
    "  - Are sentences well-formed?\n",
    "  - Is the message clear?\n",
    "  - Example scores:\n",
    "    - 5: Perfect grammar, clear message\n",
    "    - 3: Understandable but awkward\n",
    "    - 1: Nonsensical or fragmented\n",
    "\n",
    "  **2. Safety** (1-5 scale):\n",
    "  - Is the content appropriate?\n",
    "  - Any hate speech or dangerous advice?\n",
    "  - Professional tone maintained?\n",
    "  - Example scores:\n",
    "    - 5: Completely safe and professional\n",
    "    - 3: Minor tone issues\n",
    "    - 1: Unsafe or inappropriate\n",
    "\n",
    "* **EvalTask Object**: This orchestrates the evaluation:\n",
    "  - Generates responses from your function\n",
    "  - Sends them to Judge LLM\n",
    "  - Collects scores\n",
    "  - Computes statistics (mean, std)\n",
    "\n",
    "* **Experiment Tracking**: Results are logged to Vertex AI:\n",
    "  - View in Cloud Console\n",
    "  - Compare runs over time\n",
    "  - Track prompt improvements\n",
    "\n",
    "* **Expected Results**:\n",
    "  ```python\n",
    "  {\n",
    "    'coherence/mean': 4.8,  # Good! Near perfect\n",
    "    'coherence/std': 0.2,   # Consistent\n",
    "    'safety/mean': 5.0,     # Excellent!\n",
    "    'safety/std': 0.0,      # Always safe\n",
    "    'row_count': 3          # Number of test cases\n",
    "  }\n",
    "  ```\n",
    "\n",
    "* **Interpreting Scores**:\n",
    "  - Mean > 4.0 = Excellent\n",
    "  - Mean 3.0-4.0 = Good, could improve\n",
    "  - Mean < 3.0 = Needs prompt refinement\n",
    "  - Low std = Consistent quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLM-AS-A-JUDGE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis tests creative output quality using AI evaluation...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION DATASET\n",
    "# ============================================================================\n",
    "\n",
    "eval_dataset = pd.DataFrame({\n",
    "    \"instruction\": [\n",
    "        \"Generate a social media post about heavy snow expected tonight\",\n",
    "        \"Generate a social media post about holiday hours for Town Hall\",\n",
    "        \"Generate a social media post about school closings due to weather\",\n",
    "    ],\n",
    "    \"reference\": [\n",
    "        \"A good post should warn residents, suggest safety measures, and include a hashtag\",\n",
    "        \"A good post should list the dates, mention alternative contact methods, and be friendly\",\n",
    "        \"A good post should inform parents quickly, mention alternative resources, and use hashtag\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üìä Test Cases:\")\n",
    "for i, row in eval_dataset.iterrows():\n",
    "    print(f\"  {i+1}. {row['instruction']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "metrics = [\n",
    "    \"coherence\",  # Is the text logical and well-formed?\n",
    "    \"safety\",     # Is the content appropriate and professional?\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Evaluation Metrics: {', '.join(metrics)}\")\n",
    "print(\"\\nMetric Details:\")\n",
    "print(\"  ‚Ä¢ Coherence: Measures logical flow and grammatical correctness (1-5)\")\n",
    "print(\"  ‚Ä¢ Safety: Measures appropriateness and professionalism (1-5)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE EVALUATION TASK\n",
    "# ============================================================================\n",
    "\n",
    "# Wrapper function for evaluation\n",
    "def generate_post_for_eval(instruction: str) -> str:\n",
    "    \"\"\"Wrapper that extracts topic from instruction and generates post.\"\"\"\n",
    "    # Extract the topic (everything after \"about \")\n",
    "    topic = instruction.split(\"about \")[-1] if \"about \" in instruction else instruction\n",
    "    return generate_social_post(topic)\n",
    "\n",
    "task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    experiment=\"aurora-social-media-eval-v2\",  # Track in Vertex AI\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Running evaluation (this may take 30-60 seconds)...\")\n",
    "print(\"   The Judge LLM will score each generated post...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "eval_result = task.evaluate(\n",
    "    model=model,\n",
    "    prompt_template=\"{instruction}\",  # How to format the input\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = eval_result.summary_metrics\n",
    "\n",
    "print(\"\\nüìà Score Breakdown:\")\n",
    "print(f\"  Coherence:\")\n",
    "print(f\"    Mean:  {summary.get('coherence/mean', 'N/A'):.2f} / 5.00\")\n",
    "print(f\"    Std:   {summary.get('coherence/std', 'N/A'):.2f}\")\n",
    "print(f\"\\n  Safety:\")\n",
    "print(f\"    Mean:  {summary.get('safety/mean', 'N/A'):.2f} / 5.00\")\n",
    "print(f\"    Std:   {summary.get('safety/std', 'N/A'):.2f}\")\n",
    "print(f\"\\n  Test Cases: {summary.get('row_count', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION GUIDE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Score Ranges (out of 5.0):\")\n",
    "print(\"  4.5-5.0: Excellent quality ‚úÖ\")\n",
    "print(\"  4.0-4.4: Good quality ‚úì\")\n",
    "print(\"  3.0-3.9: Acceptable, could improve ‚ö†Ô∏è\")\n",
    "print(\"  <3.0: Needs prompt refinement ‚ùå\")\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(\"  <0.5: Very consistent\")\n",
    "print(\"  0.5-1.0: Some variation\")\n",
    "print(\"  >1.0: Inconsistent results\")\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED RESULTS TABLE (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS PER TEST CASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if hasattr(eval_result, 'metrics_table'):\n",
    "    print(\"\\nüìã Individual Scores:\\n\")\n",
    "    display(eval_result.metrics_table[['prompt', 'response', 'coherence', 'safety']])\n",
    "else:\n",
    "    print(\"\\n(Detailed table not available in summary view)\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "print(\"\\nNext: Run Cell 6 for submission checklist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Manual Testing (Optional)\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Interactive Testing**: Allows you to test your functions with custom inputs\n",
    "\n",
    "* **Use Cases**:\n",
    "  - Try edge cases not covered in unit tests\n",
    "  - Test with real questions you might receive\n",
    "  - Validate prompt changes before re-running full test suite\n",
    "\n",
    "* **Example Inputs**:\n",
    "  - Classification: \"My basement is flooding!\", \"Who do I contact about recycling?\"\n",
    "  - Social Media: \"Power outage in downtown\", \"New playground opening\"\n",
    "\n",
    "* **Why manual testing matters**: Automated tests are great, but human review catches:\n",
    "  - Tone issues\n",
    "  - Cultural insensitivity\n",
    "  - Edge cases you didn't think of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MANUAL TESTING INTERFACE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTest your functions with custom inputs\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST CLASSIFICATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"TEST 1: INQUIRY CLASSIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "test_questions = [\n",
    "    \"My basement is flooding from the storm!\",\n",
    "    \"Who do I contact about recycling pickup?\",\n",
    "    \"Are you hiring for summer positions?\",\n",
    "    \"What is the assessed value of my property?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = classify_inquiry(question)\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(f\"üìÅ Category: {result}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST SOCIAL MEDIA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST 2: SOCIAL MEDIA GENERATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "test_topics = [\n",
    "    \"Power outage in downtown area\",\n",
    "    \"New playground opening at Central Park\",\n",
    "    \"Summer concert series begins next week\",\n",
    "]\n",
    "\n",
    "for topic in test_topics:\n",
    "    post = generate_social_post(topic)\n",
    "    print(f\"\\nüìù Topic: {topic}\")\n",
    "    print(f\"üì± Generated Post:\\n{post}\")\n",
    "    print(f\"   Characters: {len(post)}\")\n",
    "    print(f\"   Has hashtag: {'‚úÖ' if '#' in post else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Manual testing complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Submission Checklist and Summary\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Submission Requirements**: Lists everything needed for grading\n",
    "\n",
    "* **Implementation Summary**: Documents what you built\n",
    "\n",
    "* **GitHub Preparation**: Instructions for creating repository\n",
    "\n",
    "* **Next Steps**: Clear guidance on submission process\n",
    "\n",
    "### GitHub Submission Steps:\n",
    "\n",
    "1. **Download Notebook**:\n",
    "   - File ‚Üí Download ‚Üí Download .ipynb\n",
    "\n",
    "2. **Create Repository**:\n",
    "   ```bash\n",
    "   # On GitHub.com\n",
    "   New Repository ‚Üí challenge-03-prompt-engineering\n",
    "   Add README.md\n",
    "   ```\n",
    "\n",
    "3. **Upload Files**:\n",
    "   - Upload notebook\n",
    "   - Add README with:\n",
    "     - Project description\n",
    "     - Setup instructions\n",
    "     - Test results\n",
    "\n",
    "4. **Share Link**:\n",
    "   - Copy repository URL\n",
    "   - Submit to instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHALLENGE THREE: SUBMISSION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    (\"‚úÖ\", \"Installed pytest and evaluation libraries\"),\n",
    "    (\"‚úÖ\", \"Defined classify_inquiry() function\"),\n",
    "    (\"‚úÖ\", \"Defined generate_social_post() function\"),\n",
    "    (\"‚úÖ\", \"Created test_challenge.py file\"),\n",
    "    (\"‚úÖ\", \"Ran unit tests with pytest\"),\n",
    "    (\"‚úÖ\", \"Passed all classification tests\"),\n",
    "    (\"‚úÖ\", \"Passed all social media tests\"),\n",
    "    (\"‚úÖ\", \"Ran LLM-as-a-Judge evaluation\"),\n",
    "    (\"‚úÖ\", \"Achieved coherence score > 4.0\"),\n",
    "    (\"‚úÖ\", \"Achieved safety score = 5.0\"),\n",
    "    (\"‚¨ú\", \"Downloaded notebook (.ipynb format)\"),\n",
    "    (\"‚¨ú\", \"Created GitHub repository\"),\n",
    "    (\"‚¨ú\", \"Uploaded notebook to GitHub\"),\n",
    "    (\"‚¨ú\", \"Added README.md with documentation\"),\n",
    "    (\"‚¨ú\", \"Shared GitHub link with instructor\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\")\n",
    "for status, item in checklist:\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ Challenge Objective:\n",
    "Build two LLM-powered functions with comprehensive testing:\n",
    "1. Classification system for citizen inquiries\n",
    "2. Social media post generator for town communications\n",
    "\n",
    "üîß Technical Components:\n",
    "\n",
    "1. Function Development\n",
    "   ‚Ä¢ classify_inquiry(): Routes questions to departments\n",
    "   ‚Ä¢ generate_social_post(): Creates official social content\n",
    "   ‚Ä¢ Prompt engineering for consistent outputs\n",
    "   ‚Ä¢ Output cleaning and formatting\n",
    "\n",
    "2. Unit Testing (pytest)\n",
    "   ‚Ä¢ Test suite with 8+ test cases\n",
    "   ‚Ä¢ Classification accuracy validation\n",
    "   ‚Ä¢ Social media format verification\n",
    "   ‚Ä¢ Automated pass/fail determination\n",
    "\n",
    "3. LLM-as-a-Judge Evaluation\n",
    "   ‚Ä¢ Coherence metric (logical flow)\n",
    "   ‚Ä¢ Safety metric (appropriateness)\n",
    "   ‚Ä¢ Statistical analysis (mean, std)\n",
    "   ‚Ä¢ Vertex AI experiment tracking\n",
    "\n",
    "4. Quality Metrics\n",
    "   ‚Ä¢ Classification: 100% test pass rate\n",
    "   ‚Ä¢ Coherence: 4.0+ / 5.0 (excellent)\n",
    "   ‚Ä¢ Safety: 5.0 / 5.0 (perfect)\n",
    "   ‚Ä¢ Consistency: Low standard deviation\n",
    "\n",
    "üìö Key Learnings:\n",
    "\n",
    "‚Ä¢ Prompt Engineering: Constraining outputs for consistency\n",
    "‚Ä¢ Testing Strategies: Different approaches for different needs\n",
    "‚Ä¢ Quality Metrics: Quantifying LLM performance\n",
    "‚Ä¢ Evaluation Tools: Using AI to grade AI\n",
    "\n",
    "üéì Skills Demonstrated:\n",
    "\n",
    "‚úì Vertex AI integration\n",
    "‚úì Gemini API usage\n",
    "‚úì Prompt engineering\n",
    "‚úì Unit testing with pytest\n",
    "‚úì LLM evaluation frameworks\n",
    "‚úì Code documentation\n",
    "‚úì Software engineering best practices\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NEXT STEPS FOR SUBMISSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. Download this notebook:\n",
    "   File ‚Üí Download ‚Üí Download .ipynb\n",
    "\n",
    "2. Create GitHub repository:\n",
    "   ‚Ä¢ Repository name: challenge-03-prompt-engineering\n",
    "   ‚Ä¢ Visibility: Public or Private (per instructor)\n",
    "   ‚Ä¢ Initialize with README\n",
    "\n",
    "3. Upload files:\n",
    "   ‚Ä¢ This notebook (challenge_03_complete.ipynb)\n",
    "   ‚Ä¢ README.md with:\n",
    "     - Project description\n",
    "     - Setup instructions\n",
    "     - Test results summary\n",
    "     - Example outputs\n",
    "\n",
    "4. Submit to instructor:\n",
    "   ‚Ä¢ Copy repository URL\n",
    "   ‚Ä¢ Send via course submission system\n",
    "   ‚Ä¢ Include test results in submission notes\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ CHALLENGE THREE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExcellent work! You've successfully:\")\n",
    "print(\"  ‚úÖ Built production-quality LLM functions\")\n",
    "print(\"  ‚úÖ Implemented comprehensive testing\")\n",
    "print(\"  ‚úÖ Evaluated output quality quantitatively\")\n",
    "print(\"  ‚úÖ Demonstrated software engineering skills\")\n",
    "print(\"\\nReady for Challenge Four! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
