{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Two: RAG Chatbot with Vector Search\n",
    "\n",
    "**Objective:** Build a chatbot that uses Retrieval Augmented Generation (RAG) to answer questions about Aurora Bay FAQs.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "User Question\n",
    "    â†“\n",
    "[Generate Embedding]\n",
    "    â†“\n",
    "[Vector Search in BigQuery]\n",
    "    â†“\n",
    "[Retrieve Top-K Similar Q&A Pairs]\n",
    "    â†“\n",
    "[Pass Context + Question to Gemini]\n",
    "    â†“\n",
    "Generated Answer\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. âœ… Import CSV from GCS into BigQuery\n",
    "2. âœ… Generate embeddings for Q&A pairs\n",
    "3. âœ… Implement vector search\n",
    "4. âœ… Build chatbot with Gemini integration\n",
    "5. âœ… Well-documented Python code\n",
    "6. âœ… Ready for GitHub submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation & Runtime Configuration\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **`pip install`**: We need the latest Google Cloud libraries:\n",
    "  - `google-cloud-bigquery`: For database operations\n",
    "  - `google-cloud-aiplatform`: For Gemini and embeddings\n",
    "  - `db-dtypes`: Required for BigQuery data type handling\n",
    "  - `pandas`: For data manipulation\n",
    "  \n",
    "* **`kernel.do_shutdown(True)`**: This is a crucial \"hard reset.\" After installing new libraries, we must restart the runtime to load them properly. This prevents version conflicts.\n",
    "\n",
    "* **Why restart?** Once a library is loaded into Python memory, you cannot update it mid-session. The restart forces Colab to reload everything fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade --quiet google-cloud-bigquery google-cloud-aiplatform db-dtypes pandas numpy\n",
    "\n",
    "# Restart the runtime to load new libraries\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Project Configuration\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Project Setup**: We define the Google Cloud project ID and dataset/table names. Change `YOUR_PROJECT_ID` to your actual project.\n",
    "\n",
    "* **Region Configuration**: Using `us-central1` for consistent latency. All services (BigQuery, Vertex AI) should be in the same region.\n",
    "\n",
    "* **Data Source**: The GCS path `gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv` contains the FAQ data we'll import.\n",
    "\n",
    "* **Authentication**: Colab handles authentication automatically when you run this in Colab Enterprise or authenticate via `gcloud auth login`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"  # TODO: Change this to your project ID\n",
    "REGION = \"us-central1\"\n",
    "DATASET_ID = \"aurora_bay_faqs\"\n",
    "TABLE_ID = \"faq_embeddings\"\n",
    "GCS_SOURCE_URI = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
    "\n",
    "# Embedding configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-004\"  # Google's latest embedding model\n",
    "EMBEDDING_DIMENSION = 768  # Dimension of text-embedding-004\n",
    "\n",
    "# Gemini configuration\n",
    "GEMINI_MODEL = \"gemini-2.5-flash\"  # Fast, efficient model for RAG\n",
    "\n",
    "print(f\"âœ… Configuration loaded for project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Import Libraries & Initialize Clients\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **BigQuery Client**: Handles all database operations (create tables, load data, queries)\n",
    "\n",
    "* **Vertex AI**: Provides access to:\n",
    "  - `TextEmbeddingModel`: Converts text to vector embeddings\n",
    "  - `GenerativeModel`: The Gemini LLM for generating responses\n",
    "\n",
    "* **Why separate clients?** Each Google Cloud service has its own API. We initialize them once and reuse throughout the notebook.\n",
    "\n",
    "* **Error Handling**: We wrap initialization in try-except to catch authentication or permission issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from google.cloud import bigquery\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "\n",
    "# Initialize Vertex AI\n",
    "try:\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "    print(\"âœ… Vertex AI initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing Vertex AI: {e}\")\n",
    "    print(\"Make sure you've authenticated and have proper permissions\")\n",
    "\n",
    "# Initialize BigQuery client\n",
    "try:\n",
    "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "    print(\"âœ… BigQuery client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing BigQuery: {e}\")\n",
    "\n",
    "# Initialize embedding model\n",
    "try:\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL)\n",
    "    print(f\"âœ… Embedding model loaded: {EMBEDDING_MODEL}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading embedding model: {e}\")\n",
    "\n",
    "# Initialize Gemini model\n",
    "try:\n",
    "    gemini_model = GenerativeModel(GEMINI_MODEL)\n",
    "    print(f\"âœ… Gemini model loaded: {GEMINI_MODEL}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading Gemini model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Create BigQuery Dataset and Table\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Dataset Creation**: A dataset is like a folder in BigQuery. It organizes related tables.\n",
    "\n",
    "* **Table Schema**: We define the structure:\n",
    "  - `question` (STRING): The FAQ question\n",
    "  - `answer` (STRING): The FAQ answer\n",
    "  - `embedding` (FLOAT64, REPEATED): The vector embedding (array of 768 floats)\n",
    "\n",
    "* **REPEATED Field**: This is BigQuery's way of storing arrays. Each embedding is a list of numbers.\n",
    "\n",
    "* **exists_ok=True**: If the dataset already exists, don't fail - just use it.\n",
    "\n",
    "* **Why separate table?** We'll first load the CSV, then add embeddings in a separate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_and_table():\n",
    "    \"\"\"\n",
    "    Creates BigQuery dataset and table with schema for FAQ embeddings.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
    "    dataset.location = REGION\n",
    "    \n",
    "    try:\n",
    "        dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"âœ… Dataset {DATASET_ID} ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating dataset: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Define table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"question\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"answer\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\n",
    "            \"embedding\", \n",
    "            \"FLOAT64\", \n",
    "            mode=\"REPEATED\",  # Array of floats\n",
    "            description=f\"Vector embedding from {EMBEDDING_MODEL}\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Create table\n",
    "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    try:\n",
    "        table = bq_client.create_table(table, exists_ok=True)\n",
    "        print(f\"âœ… Table {TABLE_ID} ready\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating table: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute\n",
    "create_dataset_and_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load CSV Data from GCS\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **GCS URI**: The `gs://` path points directly to Google Cloud Storage. BigQuery can read from GCS natively.\n",
    "\n",
    "* **Load Job**: BigQuery jobs are asynchronous. We submit the job and wait for it to complete.\n",
    "\n",
    "* **Auto-detect**: BigQuery can automatically detect the CSV schema, but we explicitly define it for safety.\n",
    "\n",
    "* **Write Disposition**: `WRITE_TRUNCATE` means \"delete existing data and reload\". This makes the notebook re-runnable.\n",
    "\n",
    "* **Why not pandas?** For large datasets, loading directly from GCS to BigQuery is much faster than going through pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_bigquery():\n",
    "    \"\"\"\n",
    "    Loads FAQ CSV from GCS into BigQuery table.\n",
    "    \"\"\"\n",
    "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    # Configure load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,  # Skip header row\n",
    "        autodetect=False,  # Use explicit schema\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"question\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"answer\", \"STRING\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“¥ Loading data from {GCS_SOURCE_URI}...\")\n",
    "    \n",
    "    try:\n",
    "        # Start load job\n",
    "        load_job = bq_client.load_table_from_uri(\n",
    "            GCS_SOURCE_URI,\n",
    "            table_ref,\n",
    "            job_config=job_config\n",
    "        )\n",
    "        \n",
    "        # Wait for job to complete\n",
    "        load_job.result()  # Blocks until done\n",
    "        \n",
    "        # Get row count\n",
    "        table = bq_client.get_table(table_ref)\n",
    "        print(f\"âœ… Loaded {table.num_rows} rows into {TABLE_ID}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute\n",
    "load_csv_to_bigquery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Preview the Data\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Data Validation**: Before generating embeddings, we verify the data loaded correctly.\n",
    "\n",
    "* **LIMIT 5**: We only fetch 5 rows for preview. The full dataset might have hundreds.\n",
    "\n",
    "* **to_dataframe()**: Converts BigQuery results to a pandas DataFrame for easy viewing in the notebook.\n",
    "\n",
    "* **Why preview?** To check:\n",
    "  - Are questions and answers present?\n",
    "  - Is the data format correct?\n",
    "  - Any empty/null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the loaded data\n",
    "query = f\"\"\"\n",
    "SELECT question, answer\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š Sample FAQ data:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)\n",
    "\n",
    "# Get total count\n",
    "count_query = f\"\"\"\n",
    "SELECT COUNT(*) as total_rows\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "\"\"\"\n",
    "\n",
    "total = bq_client.query(count_query).to_dataframe()['total_rows'][0]\n",
    "print(f\"\\nðŸ“ˆ Total FAQ pairs: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Generate Embeddings Function\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Batch Processing**: The embedding API can process multiple texts at once (up to 250). This is much faster than one-by-one.\n",
    "\n",
    "* **Concatenation**: We combine question + answer into one text. Why?\n",
    "  - The embedding captures semantic meaning of the full Q&A pair\n",
    "  - When user asks a question, we find similar Q&A contexts\n",
    "\n",
    "* **Rate Limiting**: The `time.sleep(0.1)` prevents hitting API rate limits. In production, use exponential backoff.\n",
    "\n",
    "* **Error Handling**: If one batch fails, we continue with others rather than stopping entirely.\n",
    "\n",
    "* **Return Format**: Each embedding is a list of 768 floating-point numbers representing the semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to process per API call (max 250)\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors (each is a list of 768 floats)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Call embedding API\n",
    "            embeddings = embedding_model.get_embeddings(batch)\n",
    "            \n",
    "            # Extract vectors\n",
    "            batch_vectors = [emb.values for emb in embeddings]\n",
    "            all_embeddings.extend(batch_vectors)\n",
    "            \n",
    "            print(f\"âœ… Generated embeddings for batch {i//batch_size + 1} ({len(batch)} texts)\")\n",
    "            \n",
    "            # Rate limiting (10 requests per second max)\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in batch {i//batch_size + 1}: {e}\")\n",
    "            # Add empty embeddings for failed batch\n",
    "            all_embeddings.extend([[0.0] * EMBEDDING_DIMENSION] * len(batch))\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "print(\"âœ… Embedding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Generate and Store Embeddings\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Fetch Data**: We retrieve all questions and answers from BigQuery.\n",
    "\n",
    "* **Create Combined Text**: The `combined_text` is what gets embedded. It captures the full context of each Q&A pair.\n",
    "\n",
    "* **Generate Embeddings**: Uses the batch function defined above.\n",
    "\n",
    "* **Update Table**: We use BigQuery's `UPDATE` to add embeddings to existing rows.\n",
    "\n",
    "* **Transaction Safety**: The UPDATE happens row-by-row with matching question/answer pairs.\n",
    "\n",
    "* **Progress Tracking**: Shows progress every 10 rows so you know it's working.\n",
    "\n",
    "* **Why not load_table_from_dataframe?** We already have the base data. We're just adding embeddings to existing rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_store_embeddings():\n",
    "    \"\"\"\n",
    "    Generates embeddings for all Q&A pairs and stores them in BigQuery.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Fetching FAQ data...\")\n",
    "    \n",
    "    # Fetch all Q&A pairs\n",
    "    query = f\"\"\"\n",
    "    SELECT question, answer\n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    WHERE embedding IS NULL OR ARRAY_LENGTH(embedding) = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âœ… All rows already have embeddings!\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"ðŸ“ Processing {len(df)} FAQ pairs...\")\n",
    "    \n",
    "    # Combine question and answer for embedding\n",
    "    combined_texts = [\n",
    "        f\"Question: {row['question']}\\nAnswer: {row['answer']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ§  Generating embeddings (this may take a few minutes)...\")\n",
    "    embeddings = generate_embeddings_batch(combined_texts)\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df['embedding'] = embeddings\n",
    "    \n",
    "    print(\"ðŸ’¾ Storing embeddings in BigQuery...\")\n",
    "    \n",
    "    # Update table with embeddings\n",
    "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert embedding list to BigQuery ARRAY format\n",
    "        embedding_array = str(row['embedding']).replace('[', 'ARRAY[').replace(']', ']')\n",
    "        \n",
    "        update_query = f\"\"\"\n",
    "        UPDATE `{table_ref}`\n",
    "        SET embedding = {embedding_array}\n",
    "        WHERE question = @question AND answer = @answer\n",
    "        \"\"\"\n",
    "        \n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            query_parameters=[\n",
    "                bigquery.ScalarQueryParameter(\"question\", \"STRING\", row['question']),\n",
    "                bigquery.ScalarQueryParameter(\"answer\", \"STRING\", row['answer']),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            bq_client.query(update_query, job_config=job_config).result()\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"  âœ“ Updated {idx + 1}/{len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error updating row {idx + 1}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Successfully generated and stored embeddings for {len(df)} FAQ pairs\")\n",
    "    return True\n",
    "\n",
    "# Execute\n",
    "generate_and_store_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Vector Search Function\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Cosine Similarity**: This measures how \"similar\" two vectors are:\n",
    "  - 1.0 = Identical meaning\n",
    "  - 0.0 = Completely unrelated\n",
    "  - Formula: `dot_product(A, B) / (magnitude(A) * magnitude(B))`\n",
    "\n",
    "* **Query Embedding**: We convert the user's question into a vector using the same embedding model.\n",
    "\n",
    "* **Vector Search SQL**: BigQuery's `ARRAY` functions let us do vector math:\n",
    "  - `ARRAY_LENGTH`: Gets vector dimension\n",
    "  - `SUM(a * b)`: Dot product\n",
    "  - `SQRT(SUM(a * a))`: Vector magnitude\n",
    "\n",
    "* **ORDER BY similarity DESC**: Returns most similar Q&A pairs first.\n",
    "\n",
    "* **LIMIT top_k**: Only get the top N most relevant results. More results = more context but slower.\n",
    "\n",
    "* **Why this works**: Questions with similar semantic meaning will have similar embeddings, so their cosine similarity will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query: str, top_k: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs vector similarity search to find relevant FAQ pairs.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of most similar results to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with question, answer, and similarity score\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Searching for: '{query}'\")\n",
    "    \n",
    "    # Generate embedding for user query\n",
    "    query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "    \n",
    "    # Convert to string format for SQL\n",
    "    query_vector = str(query_embedding)\n",
    "    \n",
    "    # Vector similarity search using cosine similarity\n",
    "    search_query = f\"\"\"\n",
    "    WITH query_embedding AS (\n",
    "        SELECT {query_vector} AS query_vector\n",
    "    )\n",
    "    SELECT \n",
    "        question,\n",
    "        answer,\n",
    "        (\n",
    "            -- Cosine similarity formula\n",
    "            SELECT SUM(a * b) / (\n",
    "                SQRT(SUM(a * a)) * SQRT(SUM(b * b))\n",
    "            )\n",
    "            FROM UNNEST(embedding) AS a WITH OFFSET pos1\n",
    "            JOIN UNNEST(query_embedding.query_vector) AS b WITH OFFSET pos2\n",
    "            ON pos1 = pos2\n",
    "        ) AS similarity\n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`,\n",
    "         query_embedding\n",
    "    WHERE ARRAY_LENGTH(embedding) > 0\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT {top_k}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        results = bq_client.query(search_query).to_dataframe()\n",
    "        print(f\"âœ… Found {len(results)} relevant FAQ pairs\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in vector search: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"âœ… Vector search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: RAG Chatbot Function\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **RAG Pipeline**:\n",
    "  1. **Retrieve**: Use vector search to find relevant Q&A pairs\n",
    "  2. **Augment**: Add retrieved context to the prompt\n",
    "  3. **Generate**: Let Gemini synthesize an answer\n",
    "\n",
    "* **Context Building**: We format the retrieved Q&A pairs as structured context for Gemini.\n",
    "\n",
    "* **Prompt Engineering**:\n",
    "  - Clear instructions: \"You are a helpful assistant...\"\n",
    "  - Explicit constraints: \"ONLY use information from the context\"\n",
    "  - Fallback behavior: \"If the context doesn't contain relevant information...\"\n",
    "\n",
    "* **Why RAG?** Gemini's knowledge is limited to its training data. RAG gives it access to your specific FAQ database in real-time.\n",
    "\n",
    "* **Grounding**: By providing explicit context, we reduce hallucination and ensure factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(user_question: str, top_k: int = 3) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    RAG chatbot that retrieves relevant context and generates response.\n",
    "    \n",
    "    Args:\n",
    "        user_question: User's question\n",
    "        top_k: Number of similar FAQs to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant FAQ pairs using vector search\n",
    "    relevant_faqs = vector_search(user_question, top_k=top_k)\n",
    "    \n",
    "    if len(relevant_faqs) == 0:\n",
    "        return {\n",
    "            'answer': \"I apologize, but I couldn't find relevant information to answer your question.\",\n",
    "            'sources': [],\n",
    "            'error': 'No relevant FAQs found'\n",
    "        }\n",
    "    \n",
    "    # Step 2: Build context from retrieved Q&A pairs\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"FAQ {i+1}:\\nQ: {row['question']}\\nA: {row['answer']}\\n(Similarity: {row['similarity']:.3f})\"\n",
    "        for i, row in relevant_faqs.iterrows()\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant for Aurora Bay, answering questions based on our FAQ database.\n",
    "\n",
    "CONTEXT (Relevant FAQ pairs):\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{user_question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use ONLY the information provided in the context above to answer the question\n",
    "2. If the context contains a direct answer, provide it clearly and concisely\n",
    "3. If the context is related but doesn't fully answer the question, synthesize a helpful response\n",
    "4. If the context doesn't contain relevant information, politely say you don't have that information\n",
    "5. Be friendly and professional\n",
    "6. Do not make up information not present in the context\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 4: Generate response using Gemini\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        answer = response.text.strip()\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': relevant_faqs[['question', 'similarity']].to_dict('records'),\n",
    "            'context': context\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': f\"Error generating response: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"âœ… RAG chatbot function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Test the Chatbot\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Test Cases**: We try different types of questions to validate the chatbot works:\n",
    "  1. Exact match questions (should have very high similarity)\n",
    "  2. Paraphrased questions (semantic similarity)\n",
    "  3. Related but different questions (context understanding)\n",
    "\n",
    "* **Source Display**: We show which FAQ pairs were used, and their similarity scores. This transparency helps:\n",
    "  - Debug retrieval quality\n",
    "  - Validate that relevant context was found\n",
    "  - Build user trust (they can see the sources)\n",
    "\n",
    "* **Similarity Threshold**: Generally:\n",
    "  - > 0.8 = Very relevant\n",
    "  - 0.6-0.8 = Somewhat relevant\n",
    "  - < 0.6 = Might not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What are the parking options in Aurora Bay?\",\n",
    "    \"How do I pay for parking?\",\n",
    "    \"What restaurants are available?\",\n",
    "    \"Tell me about the beach access\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING RAG CHATBOT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"â“ Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag_chatbot(question)\n",
    "    \n",
    "    print(f\"\\nðŸ’¬ Answer:\\n{result['answer']}\")\n",
    "    \n",
    "    if result['sources']:\n",
    "        print(f\"\\nðŸ“š Sources used:\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"  {i}. {source['question'][:80]}... (similarity: {source['similarity']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Interactive Chatbot\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Interactive Loop**: Allows you to chat with the bot in real-time.\n",
    "\n",
    "* **Input Validation**: Checks for empty inputs and exit commands.\n",
    "\n",
    "* **Session Management**: The loop continues until you type 'quit'.\n",
    "\n",
    "* **User Experience**: Clear formatting makes it easy to distinguish questions from answers.\n",
    "\n",
    "* **Why interactive?** This simulates a real chatbot interface and lets you test edge cases interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat session with the RAG chatbot.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ¤– AURORA BAY FAQ CHATBOT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Ask me anything about Aurora Bay!\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nðŸ‘¤ You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nðŸ‘‹ Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        result = rag_chatbot(user_input)\n",
    "        print(f\"\\nðŸ¤– Bot: {result['answer']}\")\n",
    "        \n",
    "        # Optionally show sources\n",
    "        if result['sources']:\n",
    "            show_sources = input(\"\\n   Show sources? (y/n): \").lower()\n",
    "            if show_sources == 'y':\n",
    "                print(\"\\n   ðŸ“š Sources:\")\n",
    "                for i, source in enumerate(result['sources'], 1):\n",
    "                    print(f\"     {i}. {source['question'][:60]}... ({source['similarity']:.3f})\")\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Evaluation Metrics\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Retrieval Quality**: Measures how well vector search finds relevant FAQs.\n",
    "\n",
    "* **Average Similarity**: Should be > 0.7 for good retrieval. Lower scores mean:\n",
    "  - Embeddings aren't capturing meaning well\n",
    "  - FAQ database doesn't cover the question\n",
    "  - Query is too vague or complex\n",
    "\n",
    "* **Response Time**: Important for user experience:\n",
    "  - < 2 seconds = Excellent\n",
    "  - 2-5 seconds = Good\n",
    "  - > 5 seconds = Too slow, consider optimization\n",
    "\n",
    "* **Why measure?** In production, you'd monitor these metrics to ensure quality doesn't degrade over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_chatbot(test_questions: List[str]):\n",
    "    \"\"\"\n",
    "    Evaluates chatbot performance on test questions.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CHATBOT EVALUATION METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_time = 0\n",
    "    similarities = []\n",
    "    successful_queries = 0\n",
    "    \n",
    "    for question in test_questions:\n",
    "        start_time = time.time()\n",
    "        result = rag_chatbot(question, top_k=3)\n",
    "        query_time = time.time() - start_time\n",
    "        \n",
    "        total_time += query_time\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            successful_queries += 1\n",
    "            if result['sources']:\n",
    "                similarities.extend([s['similarity'] for s in result['sources']])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_time = total_time / len(test_questions)\n",
    "    avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n",
    "    success_rate = (successful_queries / len(test_questions)) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  â€¢ Total questions tested: {len(test_questions)}\")\n",
    "    print(f\"  â€¢ Successful queries: {successful_queries}\")\n",
    "    print(f\"  â€¢ Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"  â€¢ Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"  â€¢ Average similarity score: {avg_similarity:.3f}\")\n",
    "    print(f\"\\nâœ… Evaluation complete!\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_chatbot(test_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Export for Submission\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "* **Documentation**: Creates a summary of your implementation for grading.\n",
    "\n",
    "* **Code Quality**: Lists the functions you created and their purposes.\n",
    "\n",
    "* **Results**: Shows that your chatbot works correctly.\n",
    "\n",
    "* **Next Steps**: Instructions for GitHub submission.\n",
    "\n",
    "* **Why this matters?** Proper documentation demonstrates:\n",
    "  - Understanding of the system\n",
    "  - Professional development practices\n",
    "  - Clear communication skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHALLENGE TWO: SUBMISSION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    (\"âœ…\", \"Created Jupyter Notebook in Vertex AI Colab Enterprise\"),\n",
    "    (\"âœ…\", \"Imported aurora-bay-faqs.csv into BigQuery\"),\n",
    "    (\"âœ…\", \"Generated embeddings for Q&A pairs using text-embedding-004\"),\n",
    "    (\"âœ…\", \"Stored embeddings in BigQuery table\"),\n",
    "    (\"âœ…\", \"Implemented vector search using cosine similarity\"),\n",
    "    (\"âœ…\", \"Built RAG chatbot with Gemini 2.5 Flash\"),\n",
    "    (\"âœ…\", \"Tested chatbot with multiple questions\"),\n",
    "    (\"âœ…\", \"Added comprehensive documentation and comments\"),\n",
    "    (\"â¬œ\", \"Upload notebook to GitHub\"),\n",
    "    (\"â¬œ\", \"Share GitHub link with instructor\"),\n",
    "]\n",
    "\n",
    "for status, item in checklist:\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Key Components:\n",
    "1. BigQuery Integration\n",
    "   - Created dataset and table schema\n",
    "   - Loaded CSV from GCS\n",
    "   - Stored vector embeddings\n",
    "\n",
    "2. Embedding Generation\n",
    "   - Used Vertex AI text-embedding-004\n",
    "   - Batch processing for efficiency\n",
    "   - Combined Q&A pairs for semantic context\n",
    "\n",
    "3. Vector Search\n",
    "   - Cosine similarity calculation in BigQuery\n",
    "   - Returns top-k most relevant FAQ pairs\n",
    "   - Configurable relevance threshold\n",
    "\n",
    "4. RAG Chatbot\n",
    "   - Retrieves relevant context\n",
    "   - Augments prompt with FAQ pairs\n",
    "   - Generates accurate responses using Gemini\n",
    "\n",
    "5. Testing & Validation\n",
    "   - Multiple test cases\n",
    "   - Performance metrics\n",
    "   - Interactive chat mode\n",
    "\n",
    "Next Steps:\n",
    "1. Download this notebook (.ipynb format)\n",
    "2. Create GitHub repository\n",
    "3. Upload notebook with README\n",
    "4. Share link with instructor for grading\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ‰ CHALLENGE COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
